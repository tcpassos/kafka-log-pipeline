# Kafka Log Pipeline

Pipeline distribuÃ­do de processamento de logs Java usando Apache Kafka como backbone central de mensageria.

## ğŸ“Š Arquitetura

```mermaid
graph TB
    subgraph "ORIGEM DOS DADOS"
        L1[ğŸ“„ Logs SIGER<br/>SIGER_*.log]
        L2[ğŸ“„ Logs Crash<br/>SigerCrashReport_*.log]
    end

    subgraph "PRODUCER STACK - Coleta"
        FB1[ğŸ” Filebeat Cliente 1<br/>filebeat-cliente1]
        FB2[ğŸ” Filebeat Cliente 2<br/>filebeat-cliente2]
    end

    subgraph "KAFKA CLUSTER - Mensageria"
        ZK[ğŸ”§ Zookeeper<br/>:2181]
        K1[ğŸ“¨ Broker 1<br/>:9092]
        K2[ğŸ“¨ Broker 2<br/>:9093]
        SR[ğŸ“‹ Schema Registry<br/>:8081]
        
        subgraph "TÃ³picos Kafka"
            T1[ğŸ“¬ Topic: logs_java_siger]
            T2[ğŸ“¬ Topic: logs_java_sigercrashreport]
        end
        
        UI1[ğŸ–¥ï¸ Kafka UI<br/>:8090]
        UI2[ğŸ–¥ï¸ Control Center<br/>:9021]
    end

    subgraph "CONSUMER STACK - Processamento"
        LS[âš™ï¸ Logstash Sink<br/>Consumer Group:<br/>logstash-elastic-consumer]
        LD[ğŸ”” Logstash Discord<br/>Consumer Group:<br/>logstash-discord-consumer]
    end

    subgraph "ARMAZENAMENTO E VISUALIZAÃ‡ÃƒO"
        ES[ğŸ—„ï¸ Elasticsearch<br/>:9200<br/>Data Stream: logs-java-siger]
        KB[ğŸ“Š Kibana<br/>:5601]
        DC[ğŸ’¬ Discord<br/>Webhook Alerts]
    end

    L1 -->|Scan 30s| FB1
    L1 -->|Scan 30s| FB2
    L2 -->|Scan 30s| FB1
    L2 -->|Scan 30s| FB2

    FB1 -->|Produz| T1
    FB1 -->|Produz| T2
    FB2 -->|Produz| T1
    FB2 -->|Produz| T2

    ZK -.->|Coordena| K1
    ZK -.->|Coordena| K2
    K1 -->|ReplicaÃ§Ã£o| K2
    K1 -.-> T1
    K1 -.-> T2
    K2 -.-> T1
    K2 -.-> T2

    T1 -->|Consome| LS
    T2 -->|Consome SEVERE logs| LD
    
    LS -->|Parse & Filter| ES
    LD -->|HTTP POST| DC

    ES -->|Visualiza| KB

    K1 -.->|Monitora| UI1
    K2 -.->|Monitora| UI1
    K1 -.->|Monitora| UI2
    K2 -.->|Monitora| UI2

    classDef producer fill:#e1f5ff,stroke:#01579b,stroke-width:2px
    classDef kafka fill:#fff9c4,stroke:#f57f17,stroke-width:2px
    classDef consumer fill:#f3e5f5,stroke:#4a148c,stroke-width:2px
    classDef storage fill:#e8f5e9,stroke:#1b5e20,stroke-width:2px
    classDef logs fill:#fff3e0,stroke:#e65100,stroke-width:2px

    class L1,L2 logs
    class FB1,FB2 producer
    class ZK,K1,K2,SR,T1,T2,UI1,UI2 kafka
    class LS consumer
    class ES,KB storage
```

## ğŸ¯ VisÃ£o Geral

Sistema de coleta, transmissÃ£o e armazenamento de logs Java (SIGER) com arquitetura baseada em:
- **Filebeat**: Coleta de logs de mÃºltiplos clientes
- **Kafka**: Mensageria distribuÃ­da com alta disponibilidade
- **Logstash**: Processamento e transformaÃ§Ã£o de logs
- **Elasticsearch**: Armazenamento indexado
- **Kibana**: VisualizaÃ§Ã£o e anÃ¡lise

## ğŸš€ InÃ­cio RÃ¡pido

### Iniciar o Pipeline Completo
```bash
cd scripts
start-stack.bat
```

### Parar o Pipeline
```bash
cd scripts
stop-stack.bat
```

## ğŸ“¦ Componentes

### 1. Kafka Cluster (`kafka-cluster/`)
Infraestrutura central de mensageria com alta disponibilidade:

- **Zookeeper** (`:2181`) - CoordenaÃ§Ã£o do cluster
- **2 Brokers Kafka** (`:9092`, `:9093`) - Cluster com replicaÃ§Ã£o
  - Replication Factor: 2
  - Min In-Sync Replicas: 2
  - 6 partiÃ§Ãµes por tÃ³pico
- **Schema Registry** (`:8081`) - Gerenciamento de schemas Avro
- **Kafka Connect** (`:8083`) - Framework para conectores
- **Control Center** (`:9021`) - Interface Confluent
- **Kafka UI** (`:8090`) - Interface web alternativa
- **REST Proxy** (`:8082`) - API REST

### 2. Producer Stack (`logs-siger-producer-filebeat/`)
Coleta de logs com mÃºltiplos clientes Filebeat:

**Filebeat Instances:**
- `filebeat-cliente1` - Coleta logs do sample-logs-1
- `filebeat-cliente2` - Coleta logs do sample-logs-2

**ConfiguraÃ§Ã£o de Inputs:**
- **logs-siger-java**: Logs normais â†’ Topic `logs_java_siger`
  - Pattern: `SIGER_*_*_[0-9]*_[0-9]*_*.*`
- **logs-sigercrash-java**: Logs de crash â†’ Topic `logs_java_sigercrashreport`
  - Pattern: `SigerCrashReport_*_*_[0-9]*_[0-9]*_*.*`

**CaracterÃ­sticas:**
- Encoding CP1252 (Windows)
- Multiline parsing para stack traces Java
- Scan a cada 30s
- Roteamento dinÃ¢mico por tÃ³pico
- Particionamento por file path

### 3. Consumer Stack

#### 3.1 Elasticsearch Consumer (`logs-siger-consumer-es/`)
Processamento e armazenamento de logs:

**Logstash Sink:**
- Consome do topic `logs_java_siger`
- Consumer Group: `logstash-elastic-consumer`

**Pipeline de Processamento:**
1. **GROK Parsing** - Extrai campos estruturados:
   - `log.session_id`, `log.sequence`, `log.thread_id`
   - `log.timestamp_str`, `log.level`, `log.logger`
   - `log.message_full`

2. **Error Extraction** - Para logs SEVERE/WARNING:
   - Identifica tipo de exceÃ§Ã£o (`error.type`)
   - Captura mensagens de erro
   - Detecta "Caused by" chains

3. **Date Conversion**:
   - Formato: `YYYY-MM-dd HH:mm:ss,SSS`
   - Timezone: America/Sao_Paulo

4. **Field Cleanup** - Remove campos temporÃ¡rios

**Elasticsearch** (`:9200`)
- Data Stream: `logs-java-siger`
- Namespace: `siger`
- Dataset: `java`

**Kibana** (`:5601`)
- Interface de visualizaÃ§Ã£o e anÃ¡lise

#### 3.2 Discord Alert Consumer (`logs-siger-consumer-discord/`)
NotificaÃ§Ãµes em tempo real para Discord:

**Logstash Discord:**
- Consome do topic `logs_java_sigercrashreport`
- Consumer Group: `logstash-discord-consumer`
- **Filtro**: Apenas logs com `level = SEVERE`

**NotificaÃ§Ã£o:**
- Envia alerta via Discord Webhook
- Inclui: Cliente, Timestamp, Logger, Tipo de Erro, Mensagem
- Bot: "SIGER Alert Bot"
- Rate-limit aware (5 msgs/2s)

**Webhook URL**: Configurado via variÃ¡vel de ambiente `DISCORD_WEBHOOK_URL`

## ğŸ”„ Fluxo de Dados

### Fluxo Principal (Elasticsearch)
```
[Arquivos .log] 
    â†“ Filebeat (scan 30s, encoding CP1252, multiline parsing)
[Kafka Topic: logs_java_siger]
    â†“ Logstash Consumer
[Parse GROK â†’ Extract Errors â†’ Convert Dates â†’ Cleanup]
    â†“ Elasticsearch Output
[Data Stream: logs-java-siger]
    â†“ Query API
[Kibana Dashboard]
```

### Fluxo de Alertas (Discord)
```
[Arquivos CrashReport.log]
    â†“ Filebeat (scan 30s, multiline parsing)
[Kafka Topic: logs_java_sigercrashreport]
    â†“ Logstash Discord Consumer
[Filter: SEVERE only â†’ Parse â†’ Format Message]
    â†“ HTTP Webhook
[Discord Channel] ğŸš¨ Alerta em tempo real!
```

## ğŸ“ Formato de Log Esperado

```
ABCD 123456 789012 2024-11-17 10:30:45,123 INFO  com.example.MyClass Mensagem do log
ABCD 123456 789012 2024-11-17 10:30:46,456 SEVERE com.example.MyClass Erro!
java.lang.RuntimeException: DescriÃ§Ã£o do erro
    at com.example.MyClass.method(MyClass.java:42)
    at com.example.Main.main(Main.java:10)
Caused by: java.lang.NullPointerException
    at com.example.Helper.process(Helper.java:15)
```

## ğŸŒ Portas Expostas

| ServiÃ§o | Porta | DescriÃ§Ã£o |
|---------|-------|-----------|
| Kafka Broker 1 | 9092 | Kafka externo |
| Kafka Broker 2 | 9093 | Kafka externo (replica) |
| Zookeeper | 2181 | CoordenaÃ§Ã£o do cluster |
| Schema Registry | 8081 | API de schemas |
| Kafka Connect | 8083 | API de conectores |
| REST Proxy | 8082 | Kafka REST API |
| Kafka UI | 8090 | Interface web |
| Control Center | 9021 | Confluent Control Center |
| Elasticsearch | 9200 | API Elasticsearch |
| Kibana | 5601 | Interface de visualizaÃ§Ã£o |

## ğŸ›ï¸ Monitoramento

- **Kafka UI**: http://localhost:8090
- **Control Center**: http://localhost:9021
- **Kibana**: http://localhost:5601
- **Elasticsearch**: http://localhost:9200

## âš™ï¸ ConfiguraÃ§Ãµes Importantes

### Kafka
- **Replication Factor**: 2
- **Min In-Sync Replicas**: 2
- **PartiÃ§Ãµes**: 6 por tÃ³pico
- **Required ACKs**: 1 (Filebeat)

### Filebeat
- **Scan Frequency**: 30s
- **Close Inactive**: 1m
- **Clean Inactive**: 72h
- **Encoding**: CP1252

### Logstash
- **Memory**: 256MB-512MB
- **Codec**: JSON
- **Timezone**: America/Sao_Paulo

### Elasticsearch
- **Mode**: Single-node
- **Memory**: 512MB-1GB
- **Security**: Desabilitada (desenvolvimento)

## ğŸ“ Estrutura do Projeto

```
kafka-log-pipeline/
â”œâ”€â”€ kafka-cluster/                    # Cluster Kafka + ferramentas
â”‚   â”œâ”€â”€ docker-compose.yml
â”‚   â””â”€â”€ prometheus.yml
â”œâ”€â”€ logs-siger-producer-filebeat/     # Coleta de logs
â”‚   â”œâ”€â”€ docker-compose.yml
â”‚   â”œâ”€â”€ filebeat.yml
â”‚   â”œâ”€â”€ sample-logs-1/               # Logs cliente 1
â”‚   â””â”€â”€ sample-logs-2/               # Logs cliente 2
â”œâ”€â”€ logs-siger-consumer-es/           # Processamento e armazenamento
â”‚   â”œâ”€â”€ docker-compose.yml
â”‚   â””â”€â”€ logstash-sink/
â”‚       â””â”€â”€ pipeline/
â”‚           â””â”€â”€ logstash.conf
â”œâ”€â”€ logs-siger-consumer-discord/      # Alertas Discord
â”‚   â”œâ”€â”€ docker-compose.yml
â”‚   â”œâ”€â”€ README.md
â”‚   â””â”€â”€ logstash-discord/
â”‚       â””â”€â”€ pipeline/
â”‚           â””â”€â”€ logstash.conf
â””â”€â”€ scripts/                          # Scripts de automaÃ§Ã£o
    â”œâ”€â”€ start-stack.bat
    â””â”€â”€ stop-stack.bat
```

## ğŸ”§ Troubleshooting

### Verificar status dos containers
```powershell
docker ps
```

### Ver logs de um serviÃ§o especÃ­fico
```powershell
docker logs -f <container_name>
```

### Verificar tÃ³picos Kafka
Acesse Kafka UI em http://localhost:8090

### Verificar Ã­ndices Elasticsearch
```powershell
curl http://localhost:9200/_cat/indices?v
```

### Limpar volumes e recomeÃ§ar
```bash
cd scripts
stop-stack.bat
docker volume prune -f
start-stack.bat
```

## ğŸ¯ Casos de Uso

- **CentralizaÃ§Ã£o de Logs**: Coleta de logs de mÃºltiplos servidores/clientes
- **AnÃ¡lise em Tempo Real**: Processamento e indexaÃ§Ã£o contÃ­nua
- **Troubleshooting**: Busca e anÃ¡lise de erros com Kibana
- **Auditoria**: HistÃ³rico completo de logs com timestamps precisos
- **Monitoramento**: Dashboards de mÃ©tricas e alertas
- **Alertas CrÃ­ticos**: NotificaÃ§Ãµes instantÃ¢neas no Discord para erros SEVERE
- **Equipes DistribuÃ­das**: Alertas em canais Discord para resposta rÃ¡pida

## ğŸ“š Tecnologias

- **Apache Kafka 7.5.0** (Confluent Platform)
- **Filebeat 8.11.1** (Elastic Beats)
- **Logstash 8.11.1** (Elastic Stack)
- **Elasticsearch 8.11.1** (Elastic Stack)
- **Kibana 8.11.1** (Elastic Stack)
- **Docker & Docker Compose**

## ğŸ”’ Notas de SeguranÃ§a

âš ï¸ **AtenÃ§Ã£o**: Esta configuraÃ§Ã£o Ã© para ambiente de **desenvolvimento**. Para produÃ§Ã£o:

- Habilitar autenticaÃ§Ã£o no Elasticsearch
- Configurar TLS/SSL no Kafka
- Implementar controle de acesso (ACLs)
- Configurar backup dos volumes
- Ajustar limites de recursos
- Habilitar autenticaÃ§Ã£o no Schema Registry e Connect

## ğŸ“„ LicenÃ§a

Este projeto Ã© um exemplo educacional/demonstrativo do sistema de logs SIGER.
